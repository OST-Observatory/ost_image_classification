# Modell-Verbesserungen f√ºr Astronomische Bildklassifikation

## TODO-Liste (Priorisiert f√ºr 18 GB RAM)

### üî• HOHE PRIORIT√ÑT (RAM-schonend, gro√üer Impact)

1. **Streaming-Datenpipeline statt Voll-Ladevorgang** (Generator/`tf.data.Dataset`)
   - Implementierung: `data_pipeline.py` bzw. Anpassung von `data_loader.py` (Generator-Modus)
   - Ziel: Keine vollst√§ndigen `numpy`-Arrays aller Bilder im RAM halten
   - Ma√ünahmen: `FITS` mit `memmap=True`, Batches on-the-fly erzeugen, `prefetch` verwenden

2. **Kleinere Eingangsaufl√∂sung** (z. B. `224x224` statt `350x350`)
   - Implementierung: Parameter in `main.py` und `data_loader.py`
   - Ziel: Reduziert RAM- und Rechenbedarf signifikant

3. **On-the-fly Augmentation im `tf.data`-Pipeline (ohne Duplikate im Speicher)**
   - Implementierung: Map/augment in Pipeline, nicht via Dataset-Verdopplung
   - Ziel: Keine aufgebl√§hten Arrays; Augmentation nur beim Durchlauf

4. **Modell vereinfachen (Flatten ‚Üí GlobalAveragePooling2D, weniger Filter)**
   - Implementierung: `model.py`
   - Ziel: Deutlich weniger Aktivierungsspeicher und Parameter

5. **Kleinere Batch-Gr√∂√üe (z. B. 8) und ggf. Gradientenakkumulation**
   - Implementierung: Trainings-Loop/Callback f√ºr Akkumulation (optional)
   - Ziel: RAM-Spitzen reduzieren bei √§hnlicher effektiver Batch-Gr√∂√üe

### üü° MITTLERE PRIORIT√ÑT (Solide Speichergewinne, moderater Aufwand)

6. **Leichtgewichtiges CNN (Depthwise/Separable Convs, MobileNet-√§hnlich)**
   - Implementierung: `model.py`
   - Ziel: Geringerer Speicher- und Rechen-Footprint bei guter Genauigkeit

7. **Eingabedatentyp und Features auf `float16` reduzieren**
   - Implementierung: Cast im Loader/Pipeline; Modell ggf. weiter in `float32`
   - Ziel: Halbierung des Eingabe-Speichers; Vorsicht bei CPU-Leistung

8. **Batched Evaluation/Prediction** (keine Voll-Dataset-Inferenz im RAM)
   - Implementierung: `evaluate_model.py` in Batches evaluieren
   - Ziel: Stabil bei gro√üen Testsets

### üü¢ NIEDRIGE PRIORIT√ÑT (Mehr Aufwand, optional)

9. **TFRecords + sequentielles Streaming**
   - Implementierung: Exporter + `tf.data.TFRecordDataset`
   - Ziel: Skalierbar f√ºr sehr gro√üe Datens√§tze

10. **Hyperparameter-Optimierung (Optuna) mit RAM-Grenzen**
    - Implementierung: `hyperparameter_optimizer.py` (kleine Batches, EarlyStop, wenige Trials parallel)
    - Ziel: Tuning unter 18 GB stabil durchf√ºhren

11. **Ensembles/Transformer/Transfer Learning (ressourcenbewusst)**
    - Implementierung: Sp√§ter, wenn Pipeline stabil ist; ggf. kleinere Backbones
    - Ziel: Genauigkeit steigern, Kosten beachten

---

## Ma√ünahmen zur Genauigkeitssteigerung (RAM-neutral)

### üî• Hohe Priorit√§t (geringer Aufwand, guter Gewinn)

1. **Stratified Split + Reproduzierbarkeit**
   - Implementierung: `train_test_split(..., stratify=labels_int)` in `main.py`
   - Erwarteter Gewinn: +1‚Äì3% stabilere Genauigkeit; verl√§sslichere Evaluation
   - Aufwand: sehr gering

2. **Klassengewichte ODER Focal Loss (klassen-spezifisches alpha)**
   - Implementierung: Class Weights via `compute_class_weight` oder `focal_loss(alpha_vec, gamma)` in `model.py`
   - Erwarteter Gewinn: +3‚Äì8% f√ºr unterrepr√§sentierte/verwechselte Klassen
   - Aufwand: gering
   - Hinweis: Nicht gleichzeitig verwenden; zuerst Class Weights testen, dann ggf. Focal Loss

3. **Label Smoothing (0.05‚Äì0.1) + besseres LR-Scheduling**
   - Implementierung: `CategoricalCrossentropy(label_smoothing=0.05)`, `CosineDecayRestarts` oder `AdamW` mit `weight_decay`
   - Erwarteter Gewinn: +1‚Äì4% allgemein bessere Generalisierung
   - Aufwand: gering

4. **Gezielte On-the-fly Augmentation pro Klasse (ohne Duplikate)**
   - Implementierung: leichte Rotationen/Crops/Rauschen f√ºr `deep_sky`; konservativere Augmentation f√ºr `darks`/`spectrum_dados`
   - Erwarteter Gewinn: +2‚Äì6% (Recall-Boost f√ºr `deep_sky`, Pr√§zision bei √ºberpr√§dizierten Klassen)
   - Aufwand: gering

5. **GlobalAveragePooling2D statt Flatten**
   - Implementierung: `model.py` (CNN-Head vereinfachen)
   - Erwarteter Gewinn: +1‚Äì3% (robustere Merkmalsaggregation), RAM-neutral
   - Aufwand: gering

### üü° Mittlere Priorit√§t (moderater Aufwand, solider Gewinn)

6. **Balanced Batch Sampling**
   - Implementierung: in `tf.data` via `sample_from_datasets` oder gewichtetes Sampling je Klasse
   - Erwarteter Gewinn: +2‚Äì5%
   - Aufwand: mittel

7. **Confidence-Gating f√ºr Header-Features**
   - Implementierung: Features mit `sigmoid(a*conf+b)` gewichten, geringe/zweifelhafte Header schw√§cher einflie√üen lassen
   - Erwarteter Gewinn: +1‚Äì4% weniger Fehlklassifikation durch unzuverl√§ssige Metadaten
   - Aufwand: mittel

8. **Konsistente Feature-Standardisierung**
   - Implementierung: Skaler nur auf Trainingssplit fitten, bei Val/Test anwenden (statt reinem BN)
   - Erwarteter Gewinn: +1‚Äì3%
   - Aufwand: mittel

### üü¢ Niedrige Priorit√§t (geringer Aufwand, kleiner gezielter Gewinn)

9. **Heuristische Post-Processing-Regeln f√ºr kritische Verwechslungen**
   - Implementierung: z. B. `dark`-Pr√§diktionen verwerfen, wenn Bild-Mean/Std ungew√∂hnlich; einfache Grenzen in `evaluate_model.py`
   - Erwarteter Gewinn: +1‚Äì3% Pr√§zision bei betroffenen Klassen
   - Aufwand: gering

---

## Wenn mehr RAM verf√ºgbar: priorisierte Ma√ünahmen (Aufwand/Gewinn)

### üî• Hohe Priorit√§t (gr√∂√üerer Gewinn, moderater Aufwand)

1. **H√∂here Eingangsaufl√∂sung (z. B. 350‚Äì512 px)**
   - Gewinn: +2‚Äì6% (mehr Detail, vor allem f√ºr `deep_sky`/Spektren)
   - Aufwand: mittel; RAM-Bedarf ‚Üë

2. **MixUp/CutMix + st√§rkere Augmentation (RandAugment)**
   - Gewinn: +2‚Äì6% robustere Generalisierung
   - Aufwand: mittel; Training etwas langsamer, RAM moderat ‚Üë

3. **Gr√∂√üere Batch-Gr√∂√üe (stabilere BN-Statistiken)**
   - Gewinn: +1‚Äì3%
   - Aufwand: gering‚Äìmittel; RAM ‚Üë

4. **Umfangreicheres Hyperparameter-Tuning (Optuna/RayTune) mit vielen Trials**
   - Gewinn: +3‚Äì8%
   - Aufwand: mittel‚Äìhoch; parallelisierte Trials ‚Üí RAM/CPU ‚Üë

### üü° Mittlere Priorit√§t (sp√ºrbarer Gewinn, h√∂herer Aufwand)

5. **Transfer Learning mit gr√∂√üeren Backbones (ResNet50V2, EfficientNetV2-B0, ViT-Tiny)**
   - Gewinn: +5‚Äì15%
   - Aufwand: mittel‚Äìhoch; RAM ‚Üë, bevorzugt GPU

6. **K-Fold Training + Model Averaging**
   - Gewinn: +3‚Äì8% (robustere Modelle)
   - Aufwand: hoch; RAM/Compute ‚Üë (k Trainingsl√§ufe)

7. **Ensembles (verschiedene Seeds/Architekturen)**
   - Gewinn: +5‚Äì12%
   - Aufwand: hoch; Inferenzkosten ‚Üë, RAM ‚Üë

### üü¢ Niedrige Priorit√§t (strategisch, l√§ngerfristig)

8. **TFRecords + aggressive Caching/Prefetch**
   - Gewinn: +1‚Äì3% indirekt (mehr Epochen/konstante Pipeline m√∂glich)
   - Aufwand: mittel; RAM/Platten-I/O Management

9. **Self-Supervised Pretraining (SimCLR/BYOL) auf eigenen Daten**
   - Gewinn: +3‚Äì10% mit gen√ºgend unlabeled Data
   - Aufwand: hoch; RAM/GPU/Compute ‚Üë

10. **Reichere Dom√§nen-Features (PSF/FWHM, Stern-Dichte, spektrale Kennzahlen)**
    - Gewinn: +2‚Äì6%
    - Aufwand: mittel‚Äìhoch; Feature-Engineering + Validierung

---

## Speicherleitfaden (18 GB RAM)

- **Laden**: `fits.open(path, memmap=True)` verwenden; TIFF nur bei Bedarf vollst√§ndig lesen.
- **Pipeline**: `tf.data.Dataset.from_generator` oder `from_tensor_slices(...).map(...).batch(...).prefetch(tf.data.AUTOTUNE)`
- **Augmentation**: ausschlie√ülich on-the-fly in `map`, nicht durch Vervielfachung der Arrays.
- **Aufl√∂sung**: Start mit `224x224x1`; nur erh√∂hen, wenn n√∂tig.
- **Batch-Gr√∂√üe**: 8 (oder 4 bei Engp√§ssen); optional Gradientenakkumulation (Update nur alle N Schritte).
- **Dtype**: Eingaben/Features `float16`, Labels `int32`; Modellgewichtungen weiterhin `float32` (stabiler auf CPU).
- **Modell**: `GlobalAveragePooling2D` statt `Flatten`; Filterprogression konservativ (z. B. 24-48-96).
- **Evaluation**: Immer in Batches vorgehen; keine Gesamtdaten im RAM.

---

## Detaillierte Implementierungen

### RAM-freundliche On-the-fly Augmentation mit `tf.data`

```python
# data_pipeline.py (Skizze)
import tensorflow as tf

def augment_fn(image, features, confidences, label):
    # Beispielhafte, leichte Augmentationen
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_flip_up_down(image)
    image = tf.image.random_brightness(image, max_delta=0.05)
    return image, features, confidences, label

def build_dataset(gen_fn, *, batch_size=8, shuffle=True, cache=False, augment=True):
    ds = tf.data.Dataset.from_generator(
        gen_fn,
        output_signature=(
            tf.TensorSpec(shape=(None, None, 1), dtype=tf.float16),
            tf.TensorSpec(shape=(None,), dtype=tf.float16),
            tf.TensorSpec(shape=(4,), dtype=tf.float16),
            tf.TensorSpec(shape=(), dtype=tf.int32),
        )
    )
    if shuffle:
        ds = ds.shuffle(1024)
    if cache:
        ds = ds.cache()
    # Resize sp√§t (im Pipeline), um I/O zu reduzieren
    def _resize(image, features, confidences, label):
        image = tf.image.resize(image, (224, 224))
        return image, features, confidences, label
    ds = ds.map(_resize, num_parallel_calls=tf.data.AUTOTUNE)
    if augment:
        ds = ds.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds
```

> Hinweis: Der Generator `gen_fn` liest pro Sample on-demand (FITS `memmap=True`), konvertiert zu `float16` und liefert einzelne Beispiele. Keine Sammel-Arrays im Speicher.

### Modellvereinfachung (Flatten ‚Üí GlobalAveragePooling2D)

```python
# model.py (Ausschnitt)
from tensorflow.keras import layers

def _build_image_network(self):
    image_input = layers.Input(shape=self.input_shape)
    x = layers.Conv2D(24, 3, activation='relu', padding='same')(image_input)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(2)(x)

    x = layers.Conv2D(48, 3, activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D(2)(x)

    x = layers.Conv2D(96, 3, activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.GlobalAveragePooling2D()(x)  # statt Flatten

    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    return models.Model(inputs=image_input, outputs=x)
```

### 1. Data Augmentation f√ºr astronomische Bilder

```python
# Hinweis: Bevorzugt tf.data (siehe oben). Die unten stehende Batch-Verdopplung
# aus der urspr√ºnglichen Skizze sollte bei 18 GB RAM NICHT verwendet werden.
```

### 2. Hyperparameter-Optimierung
- Bitte unter RAM-Grenzen betreiben: kleine Batch-Gr√∂√üen, maximale Epochen begrenzen, EarlyStopping aggressiv setzen, Trials seriell statt parallel.

### 3. Focal Loss Implementation

```python
# focal_loss.py
import tensorflow as tf

def focal_loss(gamma=2.0, alpha=0.25):
    """
    Focal Loss f√ºr unausgewogene Klassen
    gamma: Fokus-Parameter (h√∂her = mehr Fokus auf schwierige Beispiele)
    alpha: Gewichtung f√ºr Klassen
    """
    def focal_loss_fn(y_true, y_pred):
        # Categorical crossentropy
        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
        
        # Probability der korrekten Klasse
        pt = tf.exp(-ce)
        
        # Focal Loss
        focal_loss = alpha * tf.pow(1 - pt, gamma) * ce
        
        return tf.reduce_mean(focal_loss)
    
    return focal_loss_fn

# Verwendung in model.py:
# model.compile(
#     optimizer='adam',
#     loss=focal_loss(gamma=2.0, alpha=0.25),
#     metrics=['accuracy']
# )
```

### 4. Erweiterte Bildvorverarbeitung

```python
# enhanced_preprocessing.py
import numpy as np
from skimage import exposure, filters, restoration
from scipy import ndimage

class EnhancedPreprocessor:
    def __init__(self, target_size=(350, 350)):
        self.target_size = target_size
    
    def preprocess_image(self, image):
        """Erweiterte Vorverarbeitung f√ºr astronomische Bilder"""
        # 1. Rauschunterdr√ºckung
        image = self._denoise_image(image)
        
        # 2. Kontrastverbesserung
        image = self._enhance_contrast(image)
        
        # 3. Hintergrund-Subtraktion
        image = self._subtract_background(image)
        
        # 4. Normalisierung
        image = self._normalize_image(image)
        
        # 5. Gr√∂√üenanpassung
        image = self._resize_image(image)
        
        return image
    
    def _denoise_image(self, image):
        """Rauschunterdr√ºckung mit verschiedenen Methoden"""
        # Gaussian Filter f√ºr hochfrequentes Rauschen
        image = filters.gaussian(image, sigma=0.5)
        
        # Median Filter f√ºr Salz-und-Pfeffer-Rauschen
        image = filters.median(image)
        
        return image
    
    def _enhance_contrast(self, image):
        """Kontrastverbesserung"""
        # Histogramm-Equalisierung
        image = exposure.equalize_hist(image)
        
        # Adaptive Histogramm-Equalisierung
        image = exposure.equalize_adapthist(image, clip_limit=0.03)
        
        return image
    
    def _subtract_background(self, image):
        """Hintergrund-Subtraktion"""
        # Rolling Ball Algorithm f√ºr Hintergrund-Subtraktion
        background = ndimage.gaussian_filter(image, sigma=50)
        image = image - background
        
        return image
    
    def _normalize_image(self, image):
        """Robuste Normalisierung"""
        # Percentile-basierte Normalisierung
        p5, p95 = np.percentile(image, (5, 95))
        image = (image - p5) / (p95 - p5)
        
        # Clipping auf [0, 1]
        image = np.clip(image, 0, 1)
        
        return image
    
    def _resize_image(self, image):
        """Gr√∂√üenanpassung mit Interpolation"""
        # Bilineare Interpolation f√ºr bessere Qualit√§t
        from skimage.transform import resize
        image = resize(image, self.target_size, order=1, preserve_range=True)
        
        return image
```

### 5. Cross-Validation Implementation

```python
# cross_validation.py
from sklearn.model_selection import StratifiedKFold
import numpy as np
import tensorflow as tf

class CrossValidator:
    def __init__(self, n_splits=5, random_state=42):
        self.n_splits = n_splits
        self.random_state = random_state
        self.scores = []
        self.models = []
    
    def cross_validate(self, data_loader, model_builder, epochs=50):
        """Stratified K-Fold Cross-Validation"""
        # Daten laden
        images, features, confidences, labels = data_loader.prepare_dataset("../image_classification_training_sample")
        labels_categorical = tf.keras.utils.to_categorical(labels, num_classes=len(data_loader.classes))
        
        # Stratified K-Fold
        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)
        
        fold_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(images, labels)):
            print(f"Training fold {fold + 1}/{self.n_splits}")
            
            # Daten aufteilen
            X_train, X_val = images[train_idx], images[val_idx]
            F_train, F_val = features[train_idx], features[val_idx]
            C_train, C_val = confidences[train_idx], confidences[val_idx]
            y_train, y_val = labels_categorical[train_idx], labels_categorical[val_idx]
            
            # Modell erstellen und trainieren
            model = model_builder()
            callbacks = [
                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
                tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.2)
            ]
            
            history = model.fit(
                [X_train, F_train, C_train], y_train,
                validation_data=([X_val, F_val, C_val], y_val),
                epochs=epochs,
                batch_size=32,
                callbacks=callbacks,
                verbose=1
            )
            
            # Beste Validierungsgenauigkeit speichern
            best_score = max(history.history['val_accuracy'])
            fold_scores.append(best_score)
            self.models.append(model)
            
            print(f"Fold {fold + 1} - Best validation accuracy: {best_score:.4f}")
        
        # Ergebnisse zusammenfassen
        mean_score = np.mean(fold_scores)
        std_score = np.std(fold_scores)
        
        print(f"\nCross-Validation Results:")
        print(f"Mean accuracy: {mean_score:.4f} ¬± {std_score:.4f}")
        print(f"Individual fold scores: {[f'{score:.4f}' for score in fold_scores]}")
        
        return mean_score, std_score, self.models
```

---

## Implementierungsplan (f√ºr 18 GB RAM)

### Phase 1 (Woche 1): RAM-First
1. Streaming-Pipeline (Generator/`tf.data`) + `memmap=True`
2. Zielaufl√∂sung 224x224 festlegen; Batch-Gr√∂√üe 8
3. On-the-fly Augmentation in `tf.data` integrieren
4. `Flatten` ‚Üí `GlobalAveragePooling2D`; Filter reduzieren

### Phase 2 (Woche 2): Stabilisierung und Tuning
1. Eingabe/Features auf `float16` casten; Modell auf `float32` belassen
2. Batched Evaluation implementieren
3. Leichtgewichtiges CNN (separable Convs) testen

### Phase 3 (Woche 3+): Genauigkeit unter Budget erh√∂hen
1. RAM-bewusstes Optuna-Tuning
2. Optional: TFRecords-Pipeline
3. Optional: Kleine Ensembles oder Transfer Learning mit kleinen Backbones

---

## Monitoring und Evaluation
- Unver√§ndert, jedoch Evaluation strikt in Batches und mit Speicherprofiling (z. B. `tracemalloc`/`psutil`).

---

*Letzte Aktualisierung: 2025-08-08*
*Status: RAM-optimierter Plan aktiv* 